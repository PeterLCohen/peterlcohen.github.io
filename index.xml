<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Math for Humans on Math for Humans</title>
    <link>/</link>
    <description>Recent content in Math for Humans on Math for Humans</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Ryan Cory-Wright (based on a template provided by Phil Chodrow)</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Big Causal Inference</title>
      <link>/post/2017-11-08-causal_inference/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-11-08-causal_inference/</guid>
      <description>&lt;p&gt;On Thursday, Nov. 9th, I&amp;rsquo;ll be giving a talk to the Human Mobility and Networks lab on the elements of &lt;strong&gt;&lt;em&gt;causal inference&lt;/em&gt;&lt;/strong&gt;, the statistical toolbox for inferring and quantifying causal relationships between phenomena. While it&amp;rsquo;s easy to take a data set and show, say, that smoking is &lt;em&gt;correlated&lt;/em&gt; with lung cancer, it&amp;rsquo;s much harder to formulate a testable, mathematical theory to support the claim that smoking &lt;em&gt;causes&lt;/em&gt; lung cancer. Causal inference, as developed in its most prominent form by Judea Pearl, tackles this challenge through the threefold framework of structural causal modeling; &lt;em&gt;do&lt;/em&gt;-calculus, and causal directed acyclic graphs (DAGs).&lt;/p&gt;

&lt;p&gt;My talk on Thursday will briefly motivate and introduce these concepts, and discuss some prospects for the use of causal methods in the context of massive, passively-collected data sets. You can find my slides &lt;a href=&#34;/pdf/causal_inference.pdf&#34;&gt;here&lt;/a&gt;. Feedback is most welcome.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Causal Inference</title>
      <link>/talk/causal_inference/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/causal_inference/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Network Community Dynamics</title>
      <link>/project/adaptive_networks/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/adaptive_networks/</guid>
      <description>&lt;p&gt;Joint work with &lt;a href=&#34;http://mucha.web.unc.edu/&#34; target=&#34;_blank&#34;&gt;Peter J. Mucha&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Quick Note on the KL Divergence</title>
      <link>/post/2017-09-04-divergence/</link>
      <pubDate>Mon, 04 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-09-04-divergence/</guid>
      <description>&lt;p&gt;I’m currently thinking about a small problem that uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Variational_Bayesian_methods&#34;&gt;variational Bayesian methods&lt;/a&gt;. Variational Bayesian methods operate by minimizing a difference function between a true posterior &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and variational approximation &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;; the assumption is that &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is intractable to calculate but the approximation class to which &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; belongs allows for tractable computation. The difference function is typically taken to be the Kullback-Leibler (KL) divergence&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[D[q \|p] \triangleq \int_x q(x) \log \frac{q(x)}{p(x)}\;.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As a result, the KL divergence pops up all over the relevant calculations. While working on some of these calculations, I noticed a simple but rather nice property that I don’t see highlighted much. Let &lt;span class=&#34;math inline&#34;&gt;\(p(x,y) = p_x(x)p_y(y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q(x,y) = q_x(x)q_y(y)\)&lt;/span&gt; both be joint distributions over random variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Since they factor, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are independent under both distributions. Then,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
    \begin{aligned}
        D[q \| p] &amp;amp;= \int_{x,y} q(x,y) \log \frac{q(x,y)}{p(x,y)}  \\ 
        &amp;amp;= \int_{x,y} q_x(x)q_y(y) \log \frac{q_x(x)q_y(y)}{p_x(x)p_y(y)}  \\
        &amp;amp;= \int_{x,y} q_x(x)q_y(y) \log \frac{q_x(x)q_y(y)}{p_x(x)p_y(y)}  \\ 
        &amp;amp;= \int_{x,y} q_x(x)q_y(y)\left[\log \frac{q_x(x)}{p_x(x)} + \log \frac{q_y(y)}{p_y(y)} \right] \\ 
        &amp;amp;= \int_{x} q_x(x) \int_y q_y(y)\left[\log \frac{q_x(x)}{p_x(x)} + \log \frac{q_y(y)}{p_y(y)} \right] \\ 
        &amp;amp;= \int_x q_x(x) \left[\log \frac{q_x(x)}{p_x(x)} + D[q_y \| p_y]\right] \\ 
        &amp;amp;= D[q_x \| p_x] + D[q_y \| p_y]\;.
    \end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That is, the divergence of factorizable distributions is just the sum of divergences between the factors.&lt;/p&gt;
&lt;div id=&#34;a-better-proof-in-the-discrete-case&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Better Proof (in the Discrete Case)&lt;/h1&gt;
&lt;p&gt;However, this mass of algebra is clearly not the most insightful way to see this fact. A better proof (in the discrete case) starts with the formula for the KL divergence as a Bregman divergence&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
    D[q\| p]  = f(q) - f(p) + \langle \nabla f(p), q - p   \rangle\;,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math display&#34;&gt;\[f(q) = - H(q) = \sum_x q(x) \log q(x)\]&lt;/span&gt; is the negative of the entropy of &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;. It is well-known that entropies of independent random variables add: if &lt;span class=&#34;math inline&#34;&gt;\(q(x,y) = q_x(x)q_y(y)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(H(q) = H(q_x) + H(q_y)\)&lt;/span&gt;. So, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
    \begin{aligned}
        D[q\| p]  &amp;amp;= f(q) - f(p) + \langle \nabla f(p), q - p   \rangle \\ 
                  &amp;amp;= f(q_x) + f(q_y) - f(p_x) - f(p_y) + \langle \nabla [f(p_x) + f(p_y)], q_xq_y - p_xp_y \rangle \\
                  &amp;amp;= f(q_x) + f(q_y) - f(p_x) - f(p_y) + \langle \nabla [f(p_x), q_x - p_x \rangle + \langle \nabla [f(p_y), q_y - p_y \rangle\\ 
                  &amp;amp;= D[q_x\|p_x] + D[q_y\|p_y]\;,
    \end{aligned}
\]&lt;/span&gt; as was to be shown. The inference from the second line requires some slightly tricky reasoning – the key is that the gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(p)\)&lt;/span&gt; is with respect to all &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X} \times \mathcal{Y}\)&lt;/span&gt; components of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;; when we split up &lt;span class=&#34;math inline&#34;&gt;\(f(p) = f(p_x) + f(p_y)\)&lt;/span&gt; and compute the gradient that way, we obtain a vector many of whose components are zero – precisely marginalizing the cross-factors in the final term.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Data Science in R</title>
      <link>/post/2017-09-04-mban/</link>
      <pubDate>Mon, 04 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-09-04-mban/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.mit.edu/~zhuo/&#34; target=&#34;_blank&#34;&gt;Daisy Zhuo&lt;/a&gt; and I had the pleasure last week of leading a day-long session for MIT&amp;rsquo;s Master of Business Analytics students in which we survey a complete, &lt;a href=&#34;https://www.tidyverse.org/&#34; target=&#34;_blank&#34;&gt;tidy&lt;/a&gt; data science workflow for the &lt;code&gt;R&lt;/code&gt; programming language. If you are interested in a concise introduction to the basics, or can use these materials in your own teaching, please feel free to browse them &lt;a href=&#34;https://philchodrow.github.io/mban_orientation/data_science_intro/index.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Feedback is most welcome!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Opinionated Introduction to Information Theory</title>
      <link>/post/intro_information_theory/</link>
      <pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/intro_information_theory/</guid>
      <description>&lt;p&gt;Information theory has been one of the cornerstones of my research methodology over the previous year. Many systems scientists use information concepts in their own work, but may not be familiar with the fundamentals of the theory. If you are curious about information theory, or if you would just like a refresher on the basics, then you may enjoy these &lt;a href=&#34;https://arxiv.org/abs/1708.07459&#34; target=&#34;_blank&#34;&gt;recently-posted notes&lt;/a&gt;, which are distilled from an informal seminar I led at Northeastern’s Network Science Institute this summer.&lt;/p&gt;

&lt;p&gt;They aim to outline some elements of the information-theoretic “way of thinking,” by cutting a rapid and interesting path through some of the foundational concepts and theorems. The emphasis is on information theory as a &lt;em&gt;theory of learning.&lt;/em&gt; To read them you&amp;rsquo;ll need familiarity with elementary probability, but no other technical background. If you do read them, I&amp;rsquo;d love your feedback!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Student Testimonials</title>
      <link>/teaching_testimonials/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 -0400</pubDate>
      
      <guid>/teaching_testimonials/</guid>
      <description>

&lt;h2 id=&#34;introduction-to-data-science-in-r-fall-2017&#34;&gt;Introduction to Data Science in R, Fall 2017&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Super well organized and engaging.&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Best R session I have ever had.&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Loved the instructor! Very energetic and clear in his instructions. It was a pleasure learning with him, wish my past professors were like him!&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Phil did a great job teaching the class. I loved the pace - not too fast, not too slow.&lt;/em&gt; &lt;strong&gt;&lt;em&gt;I feel confident in R going forward now.&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;1-204r-spring-2017&#34;&gt;1.204R, Spring 2017&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Great to discuss theory, some math, and how they apply to the paper at hand. Excellent use of an hour!&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Probably the best recitation I&amp;rsquo;ve been to at MIT in 3 years.&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;advanced-topics-in-data-science-winter-2017&#34;&gt;Advanced Topics in Data Science, Winter 2017&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Phil was exceedingly knowledgeable about the material and enthusiastic about teaching it. His organization skills allowed him to effortlessly communicate a lot of material to the students.&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Phil is a gifted teacher and I look forward to attending more lectures with him.&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;A fantastic introduction to data science.&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Well-spoken, clear, and engaging.&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Awesome work! you were super engaging and made the lecture really fun! The 3 hours flew by in no time :)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Teaching</title>
      <link>/teaching/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 -0400</pubDate>
      
      <guid>/teaching/</guid>
      <description>

&lt;p&gt;I love sharing mathematics and data science with motivated students, in academia, in industry, and in the nonprofit sector. My recent teaching activities have included courses at MIT, workshops for researchers, and trainings for nonprofits.&lt;/p&gt;

&lt;p&gt;To read nice things my students have said about me, click &lt;a href=&#34;/teaching_testimonials&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;2017&#34;&gt;2017&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Instructor&lt;/strong&gt;, Fall 2017: Introduction to Data Science in &lt;code&gt;R&lt;/code&gt;. I led a session in which we go from zero to BI dashboards in three hours for incoming Master of Business Analytics Students. Materials &lt;a href=&#34;https://philchodrow.github.io/mban_orientation/data_science_intro/index.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Invited Seminar Leader&lt;/strong&gt;, Summer 2017: Information Theory in Network Science at Northeastern&amp;rsquo;s Network Science Institute. Lecture notes available on &lt;a href=&#34;https://arxiv.org/abs/1708.07459&#34; target=&#34;_blank&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Teaching Assistant&lt;/strong&gt;: MIT Course 1.204, &amp;ldquo;Computer Modeling: From Individual Mobility to Networks.&amp;rdquo; I lead a weekly recitation focusing on mathematical and statistical methods used in contemporary research in human behavior, complex systems, and city science.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Organizer:&lt;/strong&gt; &amp;ldquo;Computation in Optimization and Statistics.&amp;rdquo; A four-week, advanced course on computational tools in applied mathematics, statistics, operations research, and computational engineering, offered by and for practicing researchers. All materials from the 2017 edition are available &lt;a href=&#34;https://philchodrow.github.io/cos_2017/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instructor:&lt;/strong&gt; &amp;ldquo;Advanced Topics in Data Science.&amp;rdquo; A three-hour integrative session on navigating the circle of data science using functional programming in &lt;code&gt;R&lt;/code&gt;. &lt;a href=&#34;https://philchodrow.github.io/cos_2017/4_advanced_topics/slides.html&#34; target=&#34;_blank&#34;&gt;Slides&lt;/a&gt;, a &lt;a href=&#34;https://philchodrow.github.io/cos_2017/4_advanced_topics/notes.html&#34; target=&#34;_blank&#34;&gt;case study&lt;/a&gt;, and &lt;a href=&#34;https://github.com/PhilChodrow/cos_2017/tree/master/4_advanced_topics&#34; target=&#34;_blank&#34;&gt;source code&lt;/a&gt; are all available.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2016&#34;&gt;2016&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Group Co-Leader:&lt;/strong&gt; Workshop on Predictive Policing at the Institute for Computational and Experimental Research in Mathematics, Providence RI.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nonprofit Analytics Consultant:&lt;/strong&gt; A sequence of trainings on data analysis workflows in &lt;code&gt;R&lt;/code&gt;, quality control in analytics pipelines, and elementary programming concepts for a growing Boston-based nonprofit startup.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;pre-2015&#34;&gt;Pre-2015&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Teaching Assistant:&lt;/strong&gt; &amp;ldquo;The Philosophy of Action&amp;rdquo; with Professor Bjørn Ramberg, Universitetet i Oslo&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Teaching Assistant:&lt;/strong&gt; &amp;ldquo;The Philosophy of Plato,&amp;rdquo; with Professor Grace Ledbetter, Swarthmore College&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Senior Thesis Mentor:&lt;/strong&gt; Swarthmore College&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Writing Mentor:&lt;/strong&gt; Swarthmore College&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mathematics Academic Support:&lt;/strong&gt; Swarthmore College&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assistant Children&amp;rsquo;s Instructor:&lt;/strong&gt; Aikido Kokikai of Swarthmore&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Divergence, Entropy, Information: An Opinionated Introduction to Information Theory</title>
      <link>/publication/intro_information_theory/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/intro_information_theory/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Structure of Spatial Segregation</title>
      <link>/publication/structure-segregation-pub/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/structure-segregation-pub/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Structure of Spatial Segregation</title>
      <link>/talk/growth_lab/</link>
      <pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/growth_lab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiscale Urban Growth</title>
      <link>/project/urban_growth/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/urban_growth/</guid>
      <description>&lt;p&gt;Joint work with &lt;a href=&#34;https://scholar.google.com/citations?user=HsubcWgAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Ema Strano&lt;/a&gt;, &lt;a href=&#34;https://web.stanford.edu/~adalbert/&#34; target=&#34;_blank&#34;&gt;Adrian Albert&lt;/a&gt;, and &lt;a href=&#34;https://scholar.google.com/citations?user=YAGjro8AAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Marta C. Gonzalez&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information Geometry for Attribute-Based Community Detection in Spatial Networks</title>
      <link>/talk/netsci_2017/</link>
      <pubDate>Tue, 20 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/netsci_2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Information Measures Segregation</title>
      <link>/post/2017-06-19-info-segregation-2/</link>
      <pubDate>Mon, 19 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-06-19-info-segregation-2/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;/post/2017-03-06-info_segregation_1/&#34;&gt;Last time&lt;/a&gt;, we studied the information-theoretic idea of &lt;em&gt;entropy&lt;/em&gt; and its relationship to our intuitive concept of &lt;em&gt;diversity&lt;/em&gt;. We saw that the entropy &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; is highest precisely when all possible groups are represented equally in a population, and lowest when only one group is represented. High entropy, high diversity.&lt;/p&gt;
&lt;p&gt;Now let’s move on to an information-theoretic view of &lt;em&gt;segregation&lt;/em&gt;. It’s important to remember that segregation is a fundamentally distinct concept from diversity. We can remind ourselves of this difference by recalling the example of the tea drinkers in the cafe. There are drinkers who prefer &lt;strong&gt;&lt;font color=&#34;#543005&#34;&gt;black&lt;/font&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;font color=&#34;#D73027&#34;&gt;red&lt;/font&gt;&lt;/strong&gt;, and &lt;strong&gt;&lt;font color=&#34;#A1D76A&#34;&gt;green&lt;/font&gt;&lt;/strong&gt; teas; on three different days, the population of the cafe looked like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-06-19-info-segregation-2_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;600px&#34; height=&#34;300px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’re interested in whether or not the shop is &lt;em&gt;diverse&lt;/em&gt;, and whether or not it is &lt;em&gt;segregated&lt;/em&gt; (by room). Intuitively,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Friday&lt;/strong&gt; is neither diverse nor segregated.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Saturday&lt;/strong&gt; is diverse, but fairly unsegregated.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sunday&lt;/strong&gt; is diverse but highly segregated.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you may remember, the entropy &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; could distinguish Friday (&lt;span class=&#34;math inline&#34;&gt;\(H = 0\)&lt;/span&gt;) from Saturday and Sunday (&lt;span class=&#34;math inline&#34;&gt;\(H = 1.10\)&lt;/span&gt;), but couldn’t distinguish Saturday and Sunday from each other. For that we need more subtle ideas. Let’s dive in!&lt;/p&gt;
&lt;div id=&#34;local-entropy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Local Entropy&lt;/h1&gt;
&lt;p&gt;One way to think about segregation is just to think about diversity again – but on a lower level of organization. To see this, compare Saturday and Sunday again, &lt;em&gt;within each room&lt;/em&gt;. Computing entropies for each of the three rooms individually, on each day, we have:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Day&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Garden&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Left&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Right&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Friday&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Saturday&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.08&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.08&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sunday&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Just as we’d expect, the entropies in the individual rooms are zero on Friday and Sunday, but high on Saturday, reflecting diversity in each room.&lt;/p&gt;
&lt;p&gt;Mathematically, the entropy of each individual room is an example of &lt;em&gt;conditioned&lt;/em&gt; or &lt;em&gt;local&lt;/em&gt; entropy. To write this down, let’s allow &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; to be a random variable denoting the tea preference of a randomly selected customer, and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; be a random variable denoting where they sit in the cafe. Then, we can write the overall entropy as &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt;, and the entropy of tea preferences conditioned on sitting in the garden as as &lt;span class=&#34;math inline&#34;&gt;\(H(Y \vert X = \text{garden})\)&lt;/span&gt;. The average value of the conditioned entropy across all rooms is important enough to have a special name: the &lt;em&gt;conditional entropy&lt;/em&gt;. We can write it as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(Y\vert X) \triangleq \sum_{x \in \{\text{garden, left, right}\}} H(Y|X = x) \mathbb{P}(X = x)\;.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To calculate the conditional entropy, we compute the local entropy conditioned on each room, and then take a weighted average in which each room counts according to its “population.” This weighting is reflected in the factor &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(X = x)\)&lt;/span&gt;, which can be read as “the probability that a randomly selected patron is sitting in room &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.” In our example, this is the same as the proportion of patrons in room &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Applying the formula above to the local entropies we calculated earlier, we have that &lt;span class=&#34;math inline&#34;&gt;\(H(Y\vert X) \approx 1.09\)&lt;/span&gt; on Saturday, and is zero on Friday and Sunday.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mutual-information-measures-segregation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mutual Information Measures Segregation&lt;/h1&gt;
&lt;p&gt;Let’s summarise:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Day&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Diverse&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Segregated&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(H(Y \vert X)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Friday&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;No&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;No&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Saturday&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Yes&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;No&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sunday&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Yes&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Yes&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A pattern is emerging: if diversity is measured by &lt;em&gt;high&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt;, segregation is measured by &lt;em&gt;high&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt; and &lt;em&gt;low&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(H(Y \vert X)\)&lt;/span&gt;. The difference between &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H(Y \vert X)\)&lt;/span&gt; turns out to be a fundamental concept in information theory: the &lt;strong&gt;mutual information&lt;/strong&gt;, denoted &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ I(X,Y) \triangleq H(Y) - H(Y \vert X) \;.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Recall from last post that &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt; measures the difficulty of a guessing game in which I try to match the preferences of the cafe customers. From our discussion above, we can think of &lt;span class=&#34;math inline&#34;&gt;\(H(Y\vert X)\)&lt;/span&gt; in similar terms: its the difficulty of a guessing game in which I try to match the preferences of customers in &lt;em&gt;in a single room.&lt;/em&gt; So, &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt; can be interpreted as the (average) &lt;strong&gt;value of knowing which room I need to serve.&lt;/strong&gt; The value is high on Sunday: matching the preferences of the whole shop is hard (high entropy), but if you tell me which room I am serving, the problem is suddenly easy – I only need to accomodate one kind of customer. This phenomenon is measured by a high value of &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y) = 1.10\)&lt;/span&gt;. On Saturday, however, matching the preferences of any individual room is about as hard as matching the preferences of the whole shop: both the entropy &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt; and conditional entropy &lt;span class=&#34;math inline&#34;&gt;\(H(Y\vert X)\)&lt;/span&gt; are high, giving a low value of &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Another natural way to say this is that knowing what room I am serving is highly &lt;em&gt;informative&lt;/em&gt; on Sunday, but not on Saturday. The mutual information measures exactly how informative that knowledge really is.&lt;/p&gt;
&lt;p&gt;Let’s now cash out the role of the mutual information in the measurement of segregation. It’s possible to prove that the only case in which &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y) = 0\)&lt;/span&gt; is perecisely the one in which each room has the same distribution of tea preferences as the shop generally. This obtains on Friday and Saturday, but not Sunday. Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt; achieves its maximum value of &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt; precisely when each room has just one kind of tea drinker: the Saturday scenario. The larger &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt;, the greater role that your tea preference plays in determining what room you sit in.&lt;/p&gt;
&lt;p&gt;Indeed, &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt; turns out to be such a natural measure of segregation that it was reinvented in the sociological community some 25 years after Shannon’s seminal work founding information theory. The frequently used “Information Theory Index” of Theil and Finezza (1971)&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; is nothing more than &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y) / H(Y)\)&lt;/span&gt;; the normalization by &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt; results in a measure that takes values in &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;. Formally, what &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt; measures is &lt;em&gt;unevenness&lt;/em&gt;: the tendency of groups to be differently-distributed in space. Since Theil and Finezza, a number&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; of methodologists&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; have formulated&lt;a href=&#34;#fn4&#34; class=&#34;footnoteRef&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; a variety&lt;a href=&#34;#fn5&#34; class=&#34;footnoteRef&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; of measures,&lt;a href=&#34;#fn6&#34; class=&#34;footnoteRef&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; all of which can be viewed as normalized mutual informations.&lt;a href=&#34;#fn7&#34; class=&#34;footnoteRef&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;mutual-information-measures-dependence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mutual Information Measures Dependence&lt;/h2&gt;
&lt;p&gt;Readers with statistical background might read the above as a statement about dependence: &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt; measures the degree of statistical association between room and tea preference. Let’s step back from the context of diversity for a moment to explore this point. To do this, let’s compare &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt; to a more familiar quantity that does something similar: the correlation coefficient &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;. If you took a statistics class, you might remember linear regression, the problem of drawing a “best fit” line through a cloud of data plotted on the &lt;span class=&#34;math inline&#34;&gt;\(x-y\)&lt;/span&gt; plane. One thing you might have done was to compute “&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;,” which is more formally called the Pearson correlation coefficient. When &lt;span class=&#34;math inline&#34;&gt;\(R^2 = 1\)&lt;/span&gt;, the explanatory variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is perfectly correlated with the explanandum &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, but when &lt;span class=&#34;math inline&#34;&gt;\(R^2 = 0\)&lt;/span&gt;, there’s no linear relationship. One thing that might have been emphasized to you is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^2 = 0\)&lt;/span&gt; does not imply that the variables &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are independent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here’s a &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Correlation_examples2.svg#/media/File:Correlation_examples2.svg&#34;&gt;set of examples&lt;/a&gt; that you can view on Wikipedia: the bottom row data sets are all uncorrelated (&lt;span class=&#34;math inline&#34;&gt;\(R^2 = 0\)&lt;/span&gt;), but clearly not independent.&lt;/p&gt;
&lt;p&gt;The mutual information &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt; is something like a correlation coefficient on steroids, in the sense that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(I(X,Y) = 0\)&lt;/span&gt; &lt;strong&gt;does&lt;/strong&gt; imply that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are independent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One nice thing about this result is that it actually makes precise an intuition about probabilistic dependence that many of us share: two variables are dependent if knowing one “gives you information” about the other. &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt; quantifies this idea and makes it rigorously correct.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-fundamental-equation-of-segregation-and-diversity&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Fundamental Equation of Segregation and Diversity&lt;/h1&gt;
&lt;p&gt;Returning to segregation and diversity, we’re ready to write out a lovely and simple equation relating diversity and segregation across spatial scales. Let’s go back to the definition of &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt; above. Let’s recall first that &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt; measures the diversity of the entire shop. Next, the conditional entropy &lt;span class=&#34;math inline&#34;&gt;\(H(Y|X)\)&lt;/span&gt; is a measure of average diversity in each room. In the context of segregation studies, &lt;span class=&#34;math inline&#34;&gt;\(H(Y|X)\)&lt;/span&gt; measures &lt;strong&gt;exposure&lt;/strong&gt; – the diversity that an individual experiences in their local environment. From my individual perspective, the diversity of the entire shop doesn’t matter – just the diversity of my immediate surroundings. Finally, as we discussed above, the mutual information &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y)\)&lt;/span&gt; measures the segregation of the entire shop. If we take the definition of &lt;span class=&#34;math inline&#34;&gt;\(I(X,Y) = H(Y) - H(Y|X)\)&lt;/span&gt; and rearrange the terms, we wind up with what I think of as the fundamental equation of segregation and diversity:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Global Diversity} = \text{Local Exposure} + \text{Global Segregation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What’s striking about this equation is not surprise or complexity, but the way it so neatly organizes natural concepts into a quantitative order. The intuition here is simple: if there are many groups coexisting in a system (global diversity is high), then they must exist either in the same spaces (local exposure is high) or in difference spaces (global segregation is high). One of my favorite points about information theory is its ability to give precise quantitative formulations for common sense ideas about structure and learning; this equation is a prime example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-next&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What’s Next&lt;/h1&gt;
&lt;p&gt;Thanks to everyone who checked in for this two-part series! We covered the elements of an information-theoretic view of diversity and segregation. My aim in future posts is to discuss how to do practical computation with these measures. I’ll also discuss some of my research into how to analyze diversity and segregation at multiple spatial scales, and how to algorithmically find spatial structure in social systems.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Theil, H., &amp;amp; Finezza, A. J. (1971). A note on the measurement of racial integration of schools by means of informational concepts. Taylor and Francis.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Reardon, S. F. (2008). Measures of Ordinal Segregation. In Y. Flückiger, S. F. Reardon, &amp;amp; J. Silber (Eds.), Occupational and Residential Segregation (Research on Economic Inequality, Volume 17) (pp. 129–155).&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Reardon, S. F., &amp;amp; O’Sullivan, D. (2004). Measures of Spatial Segregation. Sociological Methodology, 34(1), 121–162. &lt;a href=&#34;http://doi.org/10.1111/j.0081-1750.2004.00150.x&#34; class=&#34;uri&#34;&gt;http://doi.org/10.1111/j.0081-1750.2004.00150.x&lt;/a&gt;&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Reardon, S. F., &amp;amp; Firebaugh, G. (2002). Measures of multigroup segregation. Sociological Methodology, 32, 33–67. &lt;a href=&#34;http://doi.org/10.1111/1467-9531.00110&#34; class=&#34;uri&#34;&gt;http://doi.org/10.1111/1467-9531.00110&lt;/a&gt;&lt;a href=&#34;#fnref4&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Jargowsky, P. A., &amp;amp; Kim, J. (2005). A measure of spatial segregation: The generalized neighborhood sorting index. National Poverty Center Working Paper Series. Retrieved from &lt;a href=&#34;http://nationalpovertycenter.net/publications/workingpaper05/paper03/jargowsky_kim_21mar2005.pdf&#34; class=&#34;uri&#34;&gt;http://nationalpovertycenter.net/publications/workingpaper05/paper03/jargowsky_kim_21mar2005.pdf&lt;/a&gt;&lt;a href=&#34;#fnref5&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;Roberto, E. (2015). Measuring Inequality and Segregation. arXiv.org, 1–26. Retrieved from &lt;a href=&#34;http://arxiv.org/abs/1508.01167&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1508.01167&lt;/a&gt;&lt;a href=&#34;#fnref6&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Roberto, E. (2016). The Spatial Context of Residential Segregation. arXiv.org, 1–27.&lt;a href=&#34;#fnref7&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why Fractal Cities?</title>
      <link>/draft/2017-03-21-why-fractal-cities/</link>
      <pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/draft/2017-03-21-why-fractal-cities/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Intellectual property note:&lt;/strong&gt; None of the images posted in these notes are my own. All are freely available for viewing without payment online, and I have linked to their sources.&lt;/p&gt;
&lt;div id=&#34;introduction-math-flies-the-coop&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction: Math Flies the Coop&lt;/h1&gt;
&lt;p&gt;Advanced mathematics is quite cool,&lt;sup&gt;[citation needed]&lt;/sup&gt; but relatively few advanced concepts have cultural currency outside the ivory tower. There are exceptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Differential geometry, which is the language of Einstein’s theory of general relativity (“The universe is curved.”)&lt;/li&gt;
&lt;li&gt;Functional analysis, which underlies most mathematical formulations of quantum mechanics. (“Until you observe it, the cat is both alive and dead.”)&lt;/li&gt;
&lt;li&gt;Statistics, which is the underlying mathematical theory of machine learning and artificial intelligence (“How can we separate the signal from the noise?”)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The concept we’ll discuss today is another concept of advanced mathematics that has successfully flown the coop: &lt;strong&gt;fractal geometry&lt;/strong&gt;. Most people have seen a picture of a fractal: when visualized, they are extraordinarily beautiful and somewhat mystical in appearance:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Mandel_zoom_07_satellite.jpg/1200px-Mandel_zoom_07_satellite.jpg&#34; /&gt; &lt;em&gt;The Mandelbrot set, a famous fractal first visualized by Benoit Mandelbrot in 1980. Image CC BY-SA 3.0 &lt;a href=&#34;https://commons.wikimedia.org/w/index.php?curid=322028&#34;&gt;Wikimedia Commons&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As we’re going to see, fractals actually play a quite fundamental role in the structure of modern urban life. It’s therefore extremely convenient that fractals have rich mathematical theory around them – in fact, they are one of the (relatively few) places where we understand how to use rigorous mathematics to analyze complex, multiscale phenomena. The reason we can do this is actually the same reason that fractals are so important: &lt;strong&gt;self-similarity.&lt;/strong&gt; In words, if you imagine looking at a fractal through a magnifiying glass, it will look roughly the same – no matter how strong your magnifying glass is. That is, fractals display &lt;em&gt;structural regularity&lt;/em&gt; on &lt;em&gt;multiple scales&lt;/em&gt;. The magic of fractals is that these two ingredients combine to produce such extraordinary complexity. Schematically,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{simple structure} + \text{multiple scales} = \text{complex structure}\;.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fractals-in-nature&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fractals in Nature&lt;/h1&gt;
&lt;p&gt;We’re ultimately going to see that &lt;strong&gt;fractals structure cities&lt;/strong&gt;, and survey a few reasons why. Before we do, let’s take a quick tour through fractals in nature. They appear in multiple places in the natural world, such as the growth patterns of certain organisms; the circulatory systems of plants and animals; and even geologic formations:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://themagichappensnow.com/wp-content/uploads/2015/08/tumblr_m2uqhqe5Ie1qbc06wo1_500.jpg&#34; width=&#34;200px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course, natural fractals don’t have the same mathematical precision of the Mandelbrot set: the pattern doesn’t extend to infinitely small scales, and the pattern isn’t perfectly self-replicating: there’s some unstructured variation in the system as well. It’s clear, however, that there are a few fundamental &lt;em&gt;organizing principles&lt;/em&gt; that structure these systems. To take romanesco broccoli, for example, the spirals have approximately the same shape, follow clear angular symmetry, and each nest hierarchically within spirals at one higher “level.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fractals-in-cities&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fractals in Cities&lt;/h1&gt;
&lt;p&gt;Intriguingly and importantly, &lt;strong&gt;cities also have fractal structure.&lt;/strong&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Consider, for example, the following maps of the London transportation network and spatial employment patterns, as measured by Michael Batty in his article “The Size, Scale, and Shape of Cities:”&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;These structures each display the characteristic self-similarity of fractals. In the case of the transportation network, low-volume transportation modes (small roads) are densely nested within higher-volume modes, such as highways and light rail. In the case of employment densities, we can observe small “blobs” of (town centers) that closely resemble the structural properties of the main “blob” of the urban core. It’s quite intriguing and suggestive to compare the road network above to the lung’s vascular system above.&lt;/p&gt;
&lt;p&gt;It’s possible to measure the fractal structure of different structures in cities. This is typically done by computing &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Fractal_dimension&#34;&gt;fractal dimension&lt;/a&gt;&lt;/strong&gt; of that structure. You may be familiar with 1-dimensional objects (lines and curves), two-dimensional objects (surfaces), and three-dimensional objects (“solids”). Fractals occupy an interesting in-between zone; indeed, one reason for their name is precisely that they have &lt;strong&gt;fract&lt;/strong&gt;ional dimension that reflects how much more detail is revealed when you look at the structure more closely.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Population distributions tend to have fractal dimension in the range &lt;span class=&#34;math inline&#34;&gt;\(1.5 \pm 0.3\)&lt;/span&gt;.&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; That is, human settlement doesn’t completely fill 2-dimensional space, but it’s not purely linear either.&lt;/li&gt;
&lt;li&gt;Road networks in the US tend to have fractal dimension in the range &lt;span class=&#34;math inline&#34;&gt;\(1.25 \pm 0.25\)&lt;/span&gt;. This makes sense if you figure that road networks would be lower-dimensional than population distributions, but not much lower dimensional (since the road network supports settlement).&lt;a href=&#34;#fn4&#34; class=&#34;footnoteRef&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The U.S. power grid has been measured to have fractal dimension approximately &lt;span class=&#34;math inline&#34;&gt;\(2.3\)&lt;/span&gt;. The complex web of interconnections, combined with the fact that grid wires are allowed to cross, enables greater complexity than either settlement or road patterns.&lt;a href=&#34;#fn5&#34; class=&#34;footnoteRef&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;why-are-cities-fractal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why are Cities Fractal?&lt;/h1&gt;
&lt;div id=&#34;fractals-are-dynamic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fractals are Dynamic&lt;/h2&gt;
&lt;p&gt;One reason cities display fractal structure is that they are characteristic examples of dynamic, constrained expansion. Cities are &lt;em&gt;dynamic&lt;/em&gt; because they tend to grow over time. Their growth is &lt;em&gt;constrained&lt;/em&gt; because there are usually infrastructural or administrative constraints on how much space they can fill. Dynamic, constrained growth is a common setting for the emergence of fractals. One model of network growth due to Song, Havlin, and Makse (2006)&lt;a href=&#34;#fn6&#34; class=&#34;footnoteRef&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; develops a model of the growth of complex networks in which a small collection of “hub” nodes (urban centers) send out “branches” in their vicinity at each time-step. Unfortunately, I wasn’t able to locate any images that I can reproduce here, but you can view an example graph generated via this process &lt;a href=&#34;http://www.nature.com/nphys/journal/v2/n4/images/nphys266-f3.jpg&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The authors show that the process they define leads to hubs that grow in proportion to their connectivity to peripheral nodes, but not other hubs. It’s not too surprising that this process leads to fractal structures, since we’ve essentially assumed a fractal generating process. But this model isn’t trivial – in the context of cities, it has two important properties.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Heuristic plausibility&lt;/strong&gt;: The idea of hubs “branching out” into new, smaller developments is a basically noncrazy description of urban development – imagine a small town center sprouting in the suburbs of a major city, and housing developments sprouting around that center.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Network robustness&lt;/strong&gt;: The authors show that their model networks have strong &lt;em&gt;robustness to targeted attacks.&lt;/em&gt; Roughly, fractal networks are much less likely to shut down when highly-connected nodes are disabled. The authors point out that this robustness property may be quite favorable in the evolution of complex systems. A prey animal with fractal vascular architecture may survive an attack by a predator that would otherwise be disabling. In the urban context, a city with fractal road infrastructure will experience fewer traffic jams when there are accidents at major intersections.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The point about the robustness of fractal network architectures raises an intriguing new perspective: fractal architecture isn’t just common – it’s beneficial, compared to alternative architectures. Let’s explore that point a bit more.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fractals-are-efficient&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fractals are Efficient&lt;/h2&gt;
&lt;p&gt;So far, we’ve been emphasizing the organic character of the growth of cities. This is important, but it’s equally important to remember that cities don’t just happen – they are planned. This planning is itself incremental, and it’s rare that planners have the opportunity to structure a city from the ground up. Nevertheless, the planned character of cities encourages us to consider “engineering” questions. How efficient are cities? How do they respond to their constraints?&lt;/p&gt;
&lt;p&gt;To talk about efficiency, we need to ask a fundamental teleological question: what are cities for? Here I’ll discuss the view of Luis Bettencourt in “The origins of scaling in cities”&lt;a href=&#34;#fn7&#34; class=&#34;footnoteRef&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;: cities are for increasing the rate of value-producing social interactions, under infrastructural and mobility constraints. This view is fairly abstract: by “value-producing social interactions” we include everything from gainful employment to inspirational chats in coffee shops. Roughly, cities will be successful when residents can interact with each other frequently, but this requires that residents be in spatial proximity – and that requires travel, which is costly. If travel times are large, residents will interact less and productivity will decrease.&lt;a href=&#34;#fn8&#34; class=&#34;footnoteRef&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If cities are for mixing, how should they be arranged in order to efficiently promote mixing? Fractally! More precisely, let’s consider the following problem: we have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; homes arranged in a circle. Our aim is to lay roads between them in such a way as to minimize average travel times between them. However, roads are costly, so we can’t just lay down as much as we want. What scheme should we use?&lt;/p&gt;
&lt;p&gt;Under some reasonable regularity assumptions, it’s possible to show that the optimal answer takes fractal form. The mathematical theory of these and similar problems is known as &lt;em&gt;optimal transport&lt;/em&gt;, which is studied by mathematicians like &lt;a href=&#34;https://www.math.ucdavis.edu/~qlxia/summary/transport.html&#34;&gt;Qinglan Xia&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The intuition behind the efficiency of fractals is fairly easy to grasp. Recall that the fundamental property of fractals is self-similarity – they reproduce the same structure on multiple spatial scales. With that in mind, let’s imagine solving an optimal transport problem in the circular figure above. Starting in the middle of the circular figure above, we place a link and decide how many nodes (people) will be served by that particular chunk of road. Now we face the problem of choosing to lay more road for that restricted set of people. This is, in structure, very much like the problem we started with, and has a similar solution. At each stage of laying road, we face a sub-problem similar to the one we started with, with similar solution structure. This is an illustration of &lt;em&gt;recursive problem solving&lt;/em&gt;, a foundational concept of computer science. In this case, the recursive structure implies that we see (approximately) similar structures at all spatial scales. The similarity isn’t perfect, since the problems to solve at different scales aren’t &lt;em&gt;exactly&lt;/em&gt; the same – for example, the angle expressing the “spread” of people to serve has changed. Regardless, the result is approximate self-similarity at multiple scales – fractal structure. Similar principles apply to other infrastructure layouts such as water and power distribution systems, both of which need to reach all residents in an efficient way. In summary, &lt;strong&gt;fractals are efficient&lt;/strong&gt;: they arise naturally when we need to connect people when such connections have costs in time or resources.&lt;/p&gt;
&lt;p&gt;Remember the natural fractals, like the human vascular system? Another way to think about this structure is as a solution to an optimal transport problem: the body needs to pump blood efficiently in such a way that transports necessary oxygen to the ensemble of human cells. It’s not surprising that evolution would select for organisms whose structure could do this efficiently, with minimal energy expended in the construction and maintenance of vascular infrastructure. Thus, the efficiency of fractals gives us tantalizing insight into why they should appear in so many biological systems.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summing-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summing Up&lt;/h1&gt;
&lt;p&gt;Fractals are found throughout nature, and play a fundamental role in the structure of living things that grow incrementally in response to internal and external constraints. Cities are a certain kind of complex “organism,” and fractals pop up again and again, both in empirical observation and in models of urban growth. Fractal structure plays an important role in the functioning of cities: it reflects the incremental, constrained structure of their growth; their need for resilience against random failures; and the value produced by efficiently connecting their residents.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;See &lt;em&gt;Fractal Cities&lt;/em&gt; by Michael Batty and Paul Longley for MUCH more detail on this point.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Shen, G. (2002). Fractal dimension and fractal growth of urbanized areas. International Journal of Geographical Information Science, 16(5), 419–437. &lt;a href=&#34;http://doi.org/10.1080/13658810210137013&#34; class=&#34;uri&#34;&gt;http://doi.org/10.1080/13658810210137013&lt;/a&gt;&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Batty, M. (2008). The size, scale, and shape of cities. Science, 319(5864), 769–771. &lt;a href=&#34;http://doi.org/10.1126/science.1151419&#34; class=&#34;uri&#34;&gt;http://doi.org/10.1126/science.1151419&lt;/a&gt;&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Lu, Y., &amp;amp; Tang, J. (2004). Fractal dimension of a transportation network and its relationship with urban growth: a study of the Dallas-Fort Worth area. Environment and Planning B: Planning and Design, 31(6), 895-911.&lt;a href=&#34;#fnref4&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Long, G., &amp;amp; Xu, C. (2009). The Fractal Dimensions of Complex Networks. Chinese Physics Letters, 26(8), 8–11.&lt;a href=&#34;#fnref5&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;Song, C., Havlin, S., &amp;amp; Makse, H. a. (2006). Origins of fractality in the growth of complex networks_nature. Nature Physics, 2(April), 275–281. &lt;a href=&#34;http://doi.org/10.1038/nphys266&#34; class=&#34;uri&#34;&gt;http://doi.org/10.1038/nphys266&lt;/a&gt;&lt;a href=&#34;#fnref6&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Bettencourt, L. M. A. (2013). The origins of scaling in cities. Science, 340(6139), 1438–41. &lt;a href=&#34;http://doi.org/10.1126/science.1235823&#34; class=&#34;uri&#34;&gt;http://doi.org/10.1126/science.1235823&lt;/a&gt;&lt;a href=&#34;#fnref7&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;It’s reasonable to object that interactions in and of themselves don’t drive productivity. On the other hand, the predictions of Bettencourt’s theory agree quite well with observed scaling behavior in cities.&lt;a href=&#34;#fnref8&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
